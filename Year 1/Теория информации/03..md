**Теория информации** - наука, изучающая количественные закономерности, связанные с получением, передачей, обработкой и хранением информации 
Основные задачи ТИ:
- Определение объёма сообщения
- Определение границ уплотнения канала связи с целью передачи максимума информации
- Разработка помехоустойчивого кодирования
- Оценка информационных потерь
Разделы:
- Кодирование источника
- Канальное кодирование
**Сообщение** - совокупность знаков, отображающих информацию
Передача - материальный носитель или физический процесс (сигнал)
Сообщение может быть функцией времени
Чаще используют электрические или оптические сигналы. Передача осуществляется путём изменения параметра сигнала в соответствие с сообщением
**Сигналы**:
Сообщения непригодны на больших расстояниях
В широком смысле - материальные носители информации
В узком смысле - физический процесс, отражающий сообщение и распространяющееся от источника к получателю с конечной скоростью
**Электрический сигнал** - носитель информации об электромагнитных процессах в цепи, в качестве которого используется ток или напряжение
Группы сигналов:
- Аналоговые
- Дискретные
- Цифровые
Виды процессов:
- Звуковые
- Электрические
- Магнитные
- Механические
- Оптические
**Виды представления сигналов**
- Временное (определяет длительность, мощность, энергию)
- Спектральное 
Для согласования с каналом наиболее важна ширина спектра сигнала
Временное и спектральное представление сигналов связаны между собой с помощью преобразования Фурье
### Теорема Котельникова
Любая непрерывная форма может быть представлена как цифровая
Любая функция $S(t)$, описывающая непрерывный сигнал, спектр котрого ограничен частотой $F_{c}$, определяется последовательностью мгновенных значений с периодичностью $\Delta{t}\le{}\frac{1}{2F_{c}}$, где $F_c$ - верхняя частота в спектре сигнала
Если на интервале $T_{c}$ функция существует, то она может быть отображения $2\cdot{T_{c}}\cdot{F_{c}}$ отсчётами с расстоянием $\Delta{t}$ друг от друга и образующими сигнальную кодовую группу. Также применимо к функциям с неограниченным спектром, если они быстро убывают за пределами $F_{c}$. Функция может быть восстановлена по её её отсчётам с легко оцениваемым приближением
Для оперативной передачи информации использую системы автоматизированной передачи информации
![[Pasted image 20230304094404.png]]
Источник и получатель - абоненты системы. Ими могут быть устройства или люди
В составе структуры системы передачи:
- Канал передачи (канал связи)
- Передатчик (преобразование сообщения абонента в сигнал)
- Приёмник (обратное преобразование)
В идеальном случае - однозначное соответствие между сообщением на обоих концах, но из-за помех оно можеть искажаться
**Качественные показатели системы передачи информации**
- Пропускная способность (наибольшее теоретическое количество информации, которое может передаваться по системе. Зависит от физических свойств канала и сигнала)
- Достоверность
- Надёжность
**Параметры канала**:
- $F_{k}$ - полоса частот, которую канал может пропустить, не внося заметного затухания сигнала
- $H_{k}$ - динамический диапозон, отношение максимального уровня сигнала к нормированному уровню помех
- $T_{k}$ - время использования канала
**Параметры сигнала:
- $F_{с}$ - ширина спектра частот сигнала
- $H_{k}$ - динамический диапозон, отношение среднего уровня сигнала к среднему уровню помех
- $T_{k}$ - время использования сигнала
Объем канала = произведение параметров канала
Объём сигнала - произведение параметров сигнала
Скорость передачи информации (биты в секунду или боды).
Бод - скорость передачи одного сигнала в секунду, вне зависимости от его объёма
**Достоверность** передача информации без искажения
**Надёжность** - правильное исполнение функций
**Аппаратура передачи данных** связывает терминальные устройства - оконечные устройства - с каналом связи
Пример - модем, сетевая карта, роутер

В составе СПИ большой протяжённости может использоваться аппаратура для усиления сигнала (повторители, коммутаторы, концентраторы)
Промежуточная аппаратура образует первичную сеть без функциональной нагрузки
Количество передаваемой сигналом информации на синтаксическом уровне пропорционально объёму сигнала.
Согласование сигнала с каналом связи и уплотнение каналов при передаче по ним сигналов от разных источников как раз и заключается в таком преобразовании параметров сигналов, чтобы необходимое условие
**Теорема Шеннона для дискретных каналов без помех**
**Первая теорема** - если пропускная способность канала без помех превышает производительность источника сообщений, т.е. $C_{k}\gt{V_{u}}$, если условие не выполняется, способа кодирования обеспечивающего сколь угодно высокую надёжность передачи сообщений нет
**Теорема Шеннона для дискретных каналов с помехами**
**Вторая теорема** - для канала с помехами существует такой способ кодирования, при котором обеспечивается безошибочная передача всех сообщений источника, если только пропускная способность канала превышает производительность источника, т.е. $C_{k}\gt{V_{u}}$
**Энтропия** - мера беспорядка системы, мера неопределённости какого-то опыта, который может иметь разные исходы
**Информация** - содержание сообщения, понижающего неопределённость некоторого опыта с неоднозначным исходом, убыль связанной с ним энтропии является количественной мерой информации
Пусть опыт $\alpha$ имеет $n$ неравновероятных исходов $A_{1}...A_{n}$, получим $$H(\alpha)=-\sum\limits^{N}_{i=1}p(A_{i})\times{\log_{2}p(A_{i})}$$
Если в результате опыта снимается неопределённость, то получено количество информации, равное энтропии
Если энтропия лишь уменьшилась, то количество полученной информации равно разности энтропии до опыта и энтропии после опыта
Если получено информации больше, чем энтропии, то информация избыточна
**Свойства энтропии**:
1. $H\le{0}$
2. $H=0\iff{}\exists{1\le{k}\le{N}}:p_{k}=1;\forall{i}\neq{k},p_{i}\neq{0}$ 
3. Энтропия наибольшая, кгда вероятности равны между собой
4. Энтропия объекта, состояния которого образуется совместной реализацией двух состояний, равна сумме энтропий исходных объектов

**Вероятностный подход**
Американский инженер Клод Шеннон
вероятности различных исходов неравновероятны
$I=\sum\limits^{N}_{i=1}p_{i}log_{2}(\frac{1}{P_{i}})$